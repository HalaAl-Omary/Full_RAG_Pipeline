{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf44IQG2XWrI"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# --- 1. Load the Retriever (MiniLM) on the CPU ---\n",
        "# We explicitly tell it to use the 'cpu'.\n",
        "# This is fast enough for a retriever and saves all our VRAM.\n",
        "retriever_model = SentenceTransformer(\n",
        "    'all-MiniLM-L6-v2',\n",
        "    device='cpu'  # Force to CPU\n",
        ")\n",
        "print(f\"‚úÖ Retriever model (MiniLM) loaded. Using device: cpu\")\n",
        "\n",
        "\n",
        "# --- 2. Load the Generator (DistilBERT) on the GPU ---\n",
        "# We check if a GPU is available and set the device index\n",
        "# 0 = first GPU, -1 = CPU\n",
        "pipeline_device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "generator_model = pipeline(\"question-answering\",\n",
        "                           model=\"distilbert-base-cased-distilled-squad\",\n",
        "                           device=pipeline_device) # Use GPU if available\n",
        "\n",
        "print(f\"‚úÖ Generator model (DistilBERT) loaded.\")\n",
        "if pipeline_device == 0:\n",
        "    print(\"   -> Running on GPU (Good!)\")\n",
        "else:\n",
        "    print(\"   -> WARNING: Running on CPU (Will be slow, but should work)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "knowledge_base = [\n",
        "    \"Buddy is a 3-year-old Golden Retriever who loves to play fetch.\",\n",
        "    \"The capital of France is Paris, which is known for the Eiffel Tower.\",\n",
        "    \"Python is an interpreted, high-level, general-purpose programming language.\",\n",
        "    \"The first person to walk on the Moon was Neil Armstrong in 1969.\",\n",
        "    \"Climate change is the long-term alteration of temperature and typical weather patterns.\"\n",
        "]\n",
        "\n",
        "print(f\"üìö Knowledge base created with {len(knowledge_base)} documents.\")"
      ],
      "metadata": {
        "id": "mdPSPRgOk7gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 4: Task 1 - Encode Knowledge\n",
        "print(\"--- Task 1: Encoding Knowledge Base ---\")\n",
        "\n",
        "knowledge_embeddings = retriever_model.encode(knowledge_base, convert_to_tensor=True)\n",
        "\n",
        "# --- Verification ---\n",
        "if 'knowledge_embeddings' in locals() and knowledge_embeddings.shape[0] == len(knowledge_base):\n",
        "    print(\"‚úÖ Success! Knowledge base has been encoded.\")\n",
        "    print(f\"   -> Embedding shape: {knowledge_embeddings.shape}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Task 1 not complete. 'knowledge_embeddings' not found or has wrong shape.\")"
      ],
      "metadata": {
        "id": "SGxrRGlClRjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Task 2: Building the Retriever ---\")\n",
        "\n",
        "def retrieve_context(query):\n",
        "    # 1. Encode the query\n",
        "    # Don't forget convert_to_tensor=True\n",
        "    query_embedding = retriever_model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "    # 2. Compute cosine similarity\n",
        "    # with all 'knowledge_embeddings'\n",
        "    cos_scores = util.pytorch_cos_sim(query_embedding, knowledge_embeddings)[0]\n",
        "\n",
        "    # 3. Find the best match\n",
        "    top_result_index = torch.argmax(cos_scores)\n",
        "\n",
        "    # 4. Return the matching document text\n",
        "    # TODO: Return the text from 'knowledge_base' at 'top_result_index'\n",
        "    return knowledge_base[top_result_index]\n",
        "\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"Testing retrieve_context('What is Python?')...\")\n",
        "retrieved = retrieve_context(\"What is Python?\")\n",
        "print(f\"   -> Retrieved: '{retrieved}'\")\n",
        "if \"Python\" in retrieved:\n",
        "    print(\"‚úÖ Success! Retriever function works.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Retriever function failed to find the right document.\")"
      ],
      "metadata": {
        "id": "NHU-d1dul-3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Task 3: Building the Generator ---\")\n",
        "\n",
        "def generate_answer(question, context):\n",
        "    # 1. Call the pipeline\n",
        "    # 'question' and 'context'\n",
        "    result = generator_model(question=question, context=context)\n",
        "\n",
        "    # 2. Return the answer\n",
        "    return result['answer']\n",
        "\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"Testing generate_answer('What is Python?', '...')...\")\n",
        "test_context = \"Python is a popular programming language.\"\n",
        "test_question = \"What is Python?\"\n",
        "answer = generate_answer(test_question, test_context)\n",
        "print(f\"   -> Question: '{test_question}'\")\n",
        "print(f\"   -> Context: '{test_context}'\")\n",
        "print(f\"   -> Answer: '{answer}'\")\n",
        "\n",
        "if \"popular programming language\" in answer:\n",
        "    print(\"‚úÖ Success! Generator function works.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Generator function failed to extract the answer.\")"
      ],
      "metadata": {
        "id": "_G7cWE_Bl_qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Task 4: Building the Full RAG Pipeline ---\")\n",
        "\n",
        "def ask_rag_pipeline(query):\n",
        "    # 1. Retrieve\n",
        "    # TODO: Call your 'retrieve_context' function\n",
        "    best_context = retrieve_context(query)\n",
        "\n",
        "    # 2. Generate\n",
        "    # TODO: Call your 'generate_answer' function\n",
        "    final_answer = generate_answer(question=query, context=best_context)\n",
        "\n",
        "    # 3. Return\n",
        "    return final_answer, best_context\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"Testing the full RAG pipeline...\")\n",
        "print(\"Query: 'What is the capital of France?'\")\n",
        "final_answer, retrieved_context = ask_rag_pipeline(\"What is the capital of France?\")\n",
        "\n",
        "print(f\"   -> Retrieved Context: '{retrieved_context}'\")\n",
        "print(f\"   -> Final Answer: '{final_answer}'\")\n",
        "\n",
        "if final_answer.lower() == \"paris\":\n",
        "    print(\"‚úÖ Success! Your RAG pipeline is working!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è RAG pipeline failed. Expected 'Paris'.\")"
      ],
      "metadata": {
        "id": "8U7Irwu6n-dB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}